{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling + Plots for Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import pandas as pd\n",
    "import anthropic\n",
    "from datetime import date\n",
    "from pathlib import Path\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import seaborn as sns\n",
    "import opinionated  # noqa\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.style.use(\"opinionated_rc\")\n",
    "\n",
    "# Load Anthropic API key\n",
    "load_dotenv(\"../\")\n",
    "\n",
    "CITY_SHORTCODE_NAME_LUT = {\n",
    "    \"AA\": \"Ann Arbor, MI\",\n",
    "    \"RO\": \"Royal Oak, MI\",\n",
    "    \"AP\": \"Alpena, MI\",\n",
    "    \"CS\": \"Cedar Springs, MI\",\n",
    "    \"GA\": \"Garden City, MI\",\n",
    "    \"IN\": \"Inkster, MI\",\n",
    "    \"JS\": \"Jackson, MI\",\n",
    "    \"LS\": \"Lansing, MI\",\n",
    "    \"LV\": \"Lathrup Village, MI\",\n",
    "    \"LN\": \"Livonia, MI\",\n",
    "    \"MH\": \"Madison Heights, MI\",\n",
    "    \"MT\": \"Manistee, MI\",\n",
    "    \"MP\": \"Memphis, MI\",\n",
    "    \"MC\": \"Mt Clemens, MI\",\n",
    "    \"PE\": \"Perry, MI\",\n",
    "    \"PR\": \"Pleasant Ridge, MI\",\n",
    "    \"RM\": \"Richmond, VA\",\n",
    "    \"SL\": \"Saline, , MI\",\n",
    "    \"SC\": \"St Clair, MI\",\n",
    "    \"SH\": \"Sterling Heights, MI\",\n",
    "    \"WT\": \"Williamston, MI\",\n",
    "    \"SEA\": \"Seattle, WA\",\n",
    "    \"OAK\": \"Oakland, CA\",\n",
    "}\n",
    "\n",
    "CITIES_OF_INTEREST = [\n",
    "    \"AA\",\n",
    "    \"RO\",\n",
    "    \"JS\",\n",
    "    \"LS\",\n",
    "    \"SEA\",\n",
    "    \"OAK\",\n",
    "]\n",
    "\n",
    "ANNOTATIONS_DIR = Path(\"../data/annotated-for-modeling/\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bold color map values\n",
    "# PALETTE_BOLD = cmaps.bold._colors\n",
    "COLORBREWER_PALETTE = np.array(\n",
    "    [\n",
    "        [27, 158, 119],  # green\n",
    "        [217, 95, 2],  # orange\n",
    "        [117, 112, 179],  # purple\n",
    "    ]\n",
    ")\n",
    "COLORBREWER_PALETTE = COLORBREWER_PALETTE / 255\n",
    "sns.set_palette(COLORBREWER_PALETTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all data to single object\n",
    "data_dfs = []\n",
    "\n",
    "\n",
    "def split_short_name_to_city_and_date(short_name: str) -> tuple[str, date]:\n",
    "    # Split the short name into city and date\n",
    "    short_code_and_date_parts = short_name.split(\"_\")\n",
    "\n",
    "    # Short code is the first part\n",
    "    short_code = short_code_and_date_parts[0]\n",
    "\n",
    "    # Date is the rest in month day two-digit-year format\n",
    "    event_date = date(\n",
    "        year=int(\"20\" + short_code_and_date_parts[-1]),\n",
    "        month=int(short_code_and_date_parts[1]),\n",
    "        day=int(short_code_and_date_parts[2]),\n",
    "    )\n",
    "\n",
    "    return short_code, event_date\n",
    "\n",
    "\n",
    "# Read all data\n",
    "for filepath in ANNOTATIONS_DIR.glob(\"*.csv\"):\n",
    "    # Read the comment data\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Lowercase all columns\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # Remove any spaces from column names and replace with \"_\"\n",
    "    df.columns = df.columns.str.replace(\" \", \"_\")\n",
    "\n",
    "    # Split the \"name\" column into \"city_short_code\" and \"date\"\n",
    "    df[\"city_short_code\"], df[\"date\"] = zip(\n",
    "        *df[\"name\"].apply(split_short_name_to_city_and_date),\n",
    "        strict=True,\n",
    "    )\n",
    "\n",
    "    # Add the city name\n",
    "    df[\"city_name\"] = df[\"city_short_code\"].map(CITY_SHORTCODE_NAME_LUT)\n",
    "\n",
    "    # Add a year-month column\n",
    "    df[\"year_month\"] = df[\"date\"].apply(lambda x: x.replace(day=1))\n",
    "\n",
    "    # Using the filename, mark if this was a \"training\" or \"inferred\" dataset\n",
    "    df[\"dataset_portion\"] = filepath.stem.split(\"_\")[-1]\n",
    "\n",
    "    # Add the truth data to the list\n",
    "    data_dfs.append(df)\n",
    "\n",
    "# Concatenate all training data\n",
    "full_data = pd.concat(data_dfs)\n",
    "\n",
    "# Cap the portions\n",
    "# Change test to validate\n",
    "\n",
    "# Replace dataset portion with standard names\n",
    "full_data[\"dataset_portion\"] = full_data[\"dataset_portion\"].replace(\n",
    "    {\"truth\": \"train\", \"pred\": \"inferred\", \"val\": \"test\"}\n",
    ")\n",
    "\n",
    "# Subset the data to only the columns we care about\n",
    "full_data = full_data[\n",
    "    [\n",
    "        \"name\",\n",
    "        \"city_short_code\",\n",
    "        \"city_name\",\n",
    "        \"date\",\n",
    "        \"year_month\",\n",
    "        \"dataset_portion\",\n",
    "        \"meeting_section\",\n",
    "        \"speaker_role\",\n",
    "        \"start\",\n",
    "        \"end\",\n",
    "        \"text\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# full_data.sample(3)\n",
    "full_data.loc[full_data[\"name\"].str.contains(\"OAK_02_16_21\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only the cities of interest\n",
    "full_data = full_data[full_data[\"city_short_code\"].isin(CITIES_OF_INTEREST)]\n",
    "\n",
    "# Filter out government comments and only use public comment not hearing\n",
    "meeting_comments = full_data[\n",
    "    (full_data[\"meeting_section\"] == \"Public Comment\")\n",
    "    & (full_data[\"speaker_role\"] == \"Commenter\")\n",
    "]\n",
    "\n",
    "meeting_comments.shape\n",
    "\n",
    "# Order data by city population and date\n",
    "city_order = [\n",
    "    \"Seattle, WA\",  # 737,015\n",
    "    \"Oakland, CA\",  # 440,646\n",
    "    \"Ann Arbor, MI\",  # 123,851\n",
    "    \"Lansing, MI\",  # 112,644\n",
    "    \"Royal Oak, MI\",  # 58,211\n",
    "    \"Jackson, MI\",  # 31,309\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments by City by Dataset Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the percent of comments by city (per month)\n",
    "# i.e. how much of the training or inferred data comes from this month\n",
    "\n",
    "# First get the total number of comments per city\n",
    "city_comment_counts = meeting_comments[\"city_name\"].value_counts()\n",
    "\n",
    "# Get the total number of comments per city per month\n",
    "city_month_comment_counts = meeting_comments.groupby([\"city_name\", \"year_month\"]).size()\n",
    "\n",
    "# Iter rows and calculate the percent\n",
    "percent_comments_list = []\n",
    "for _, row in meeting_comments.iterrows():\n",
    "    this_city_total_comments = city_comment_counts[row[\"city_name\"]]\n",
    "    this_city_month_total_comments = city_month_comment_counts[\n",
    "        row[\"city_name\"], row[\"year_month\"]\n",
    "    ]\n",
    "    percent = (this_city_month_total_comments / this_city_total_comments) * 100\n",
    "    percent_comments_list.append(\n",
    "        {\n",
    "            \"name\": row[\"name\"],\n",
    "            \"city_name\": row[\"city_name\"],\n",
    "            \"year_month\": row[\"year_month\"],\n",
    "            \"date\": row[\"date\"],\n",
    "            \"percent_of_comments\": percent,\n",
    "            \"Dataset Portion\": row[\"dataset_portion\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Convert to a DataFrame\n",
    "processed_percent_comments = pd.DataFrame(percent_comments_list)\n",
    "\n",
    "# For each city, order the months by date\n",
    "city_dfs = []\n",
    "for city in city_order:\n",
    "    city_df = processed_percent_comments[\n",
    "        processed_percent_comments[\"city_name\"] == city\n",
    "    ]\n",
    "    city_df = city_df.sort_values(\"year_month\", ascending=True)\n",
    "    city_dfs.append(city_df)\n",
    "\n",
    "# Concatenate the city dataframes\n",
    "percent_comments_df = pd.concat(city_dfs)\n",
    "\n",
    "# Col by city\n",
    "g = sns.catplot(\n",
    "    data=percent_comments_df,\n",
    "    x=\"date\",\n",
    "    y=\"percent_of_comments\",\n",
    "    hue=\"Dataset Portion\",\n",
    "    col=\"city_name\",\n",
    "    col_wrap=3,\n",
    "    kind=\"bar\",\n",
    ")\n",
    "\n",
    "# Update the x-axis, and y-axis labels\n",
    "g.set_axis_labels(\"Month\", \"Percent of Total Public Comments\")\n",
    "g.figure.autofmt_xdate()\n",
    "g.set_titles(\"\")\n",
    "g.set_titles(\"{col_name}\", loc=\"right\")\n",
    "\n",
    "# For each ax in the figure, set the x-axis locator to max 8 ticks\n",
    "for ax in g.axes:\n",
    "    ax.xaxis.set_major_locator(mtick.MaxNLocator(6))\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))\n",
    "\n",
    "# Tight Layout\n",
    "g.figure.tight_layout()\n",
    "\n",
    "# Move the legend outside and to the right of the plot\n",
    "sns.move_legend(g, loc=\"upper right\", bbox_to_anchor=(1.1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_comments_df.loc[(percent_comments_df[\"city_name\"] == \"Oakland, CA\")][\n",
    "    [\"name\", \"year_month\", \"Dataset Portion\"]\n",
    "].value_counts().to_frame().sort_values(\"name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC_SEEDS = {\n",
    "    \"Housing and Urban Development\": [\n",
    "        \"zoning\",\n",
    "        \"construction\",\n",
    "        \"redevelopment\",\n",
    "        \"growth\",\n",
    "        \"planning\",\n",
    "        \"housing\",\n",
    "        \"rent\",\n",
    "        \"single family\",\n",
    "        \"duplex\",\n",
    "        \"apartment\",\n",
    "    ],\n",
    "    \"Transportation and Mobility\": [\n",
    "        \"public transit\",\n",
    "        \"traffic\",\n",
    "        \"bus\",\n",
    "        \"car\",\n",
    "        \"bike lanes\",\n",
    "        \"pedestrian\",\n",
    "        \"parking\",\n",
    "    ],\n",
    "    \"Public Safety and Law Enforcement\": [\n",
    "        \"police\",\n",
    "        \"crime\",\n",
    "        \"emergency\",\n",
    "        \"safety\",\n",
    "        \"property\",\n",
    "        \"theft\",\n",
    "        \"violence\",\n",
    "        \"gun\",\n",
    "    ],\n",
    "    \"Environment and Sustainability\": [\n",
    "        \"climate\",\n",
    "        \"green\",\n",
    "        \"conservation\",\n",
    "        \"energy\",\n",
    "    ],\n",
    "    \"Homelessness\": [\n",
    "        \"homeless\",\n",
    "        \"eviction\",\n",
    "        \"shelter\",\n",
    "        \"outreach\",\n",
    "        \"mental health\",\n",
    "        \"substance abuse\",\n",
    "    ],\n",
    "    \"Parks and Recreation\": [\n",
    "        \"parks\",\n",
    "        \"outdoors\",\n",
    "        \"community\",\n",
    "        \"events\",\n",
    "        \"greenspace\",\n",
    "    ],\n",
    "    \"Economic Development\": [\n",
    "        \"business\",\n",
    "        \"jobs\",\n",
    "        \"tax\",\n",
    "        \"revitalization\",\n",
    "    ],\n",
    "    \"Arts and Culture\": [\n",
    "        \"events\",\n",
    "        \"festivals\",\n",
    "        \"museums\",\n",
    "        \"performances\",\n",
    "        \"sculpture\",\n",
    "        \"public art\",\n",
    "    ],\n",
    "    \"Education and Youth Services\": [\n",
    "        \"schools\",\n",
    "        \"libraries\",\n",
    "        \"programs\",\n",
    "        \"youth\",\n",
    "        \"kids\",\n",
    "        \"students\",\n",
    "    ],\n",
    "    \"Governance and Civic Engagement\": [\n",
    "        \"transparency\",\n",
    "        \"public participation\",\n",
    "        \"elections\",\n",
    "        \"accountability\",\n",
    "    ],\n",
    "    \"Israel-Palestine\": [\n",
    "        \"Israel\",\n",
    "        \"Palestine\",\n",
    "        \"genocide\",\n",
    "        \"Hamas\",\n",
    "        \"Jewish\",\n",
    "        \"Muslim\",\n",
    "        \"discrimination\",\n",
    "    ],\n",
    "    \"Police Reform\": [\n",
    "        \"accountability\",\n",
    "        \"community oversight\",\n",
    "        \"training\",\n",
    "        \"defund\",\n",
    "        \"reform\",\n",
    "        \"descrimination\",\n",
    "        \"racist\",\n",
    "    ],\n",
    "    \"Utilities\": [\n",
    "        \"water\",\n",
    "        \"electricity\",\n",
    "        \"sewage\",\n",
    "        \"internet\",\n",
    "        \"utilities\",\n",
    "        \"services\",\n",
    "        \"waste\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic(\n",
    "    api_key=os.environ[\"ANTHROPIC_API_KEY\"],\n",
    ")\n",
    "\n",
    "SYSTEM_MESSAGE_PROMPT = \"\"\"\n",
    "You are assisting a computational social science researcher by classifying public comments from city council meetings into various topics. The individual comment to be classified, and the possible topical classifications will be provided to you. Always format your response in XML.\n",
    "\"\"\".strip()  # noqa: E501\n",
    "\n",
    "USER_MESSAGE_PROMPT = \"\"\"\n",
    "### Context\n",
    "\n",
    "You are tasked with classifying a public comment into a single topic from a provided list.\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. First, review the list of topics.\n",
    "\n",
    "2. Next, carefully read the public comment.\n",
    "\n",
    "3. Classify this comment into exactly one of the topics from the provided list. Consider the main theme or subject of the comment and how it aligns with the available topics.\n",
    "\n",
    "4. Before making your final classification, provide your reasoning in two sentences. Explain why you believe the comment fits best into the topic you've chosen. Include this reasoning within <reasoning> tags.\n",
    "\n",
    "5. After providing your reasoning, state your final classification. Choose only one topic from the list provided, or, if none of the topics are appropriate, choose \"Other\".\n",
    "\n",
    "Remember:\n",
    "- Choose only one topic for classification.\n",
    "- Provide clear reasoning for your choice.\n",
    "- Ensure your classification is based solely on the content of the public comment and the provided list of topics, or, if necessary, choose \"Other\".\n",
    "\n",
    "Here is an example response structure:\n",
    "\n",
    "<classification-response>\n",
    "    <reasoning>...</reasoning>\n",
    "    <topic>...</topic>\n",
    "</classification-response>\n",
    "\n",
    "### Topic Information\n",
    "\n",
    "{topic_list_str}\n",
    "\n",
    "### Public Comment\n",
    "\n",
    "{public_comment}\n",
    "\n",
    "### Classification\n",
    "\n",
    "\"\"\".strip()  # noqa: E501\n",
    "\n",
    "TOPIC_SEED_STR = \"\"\"\n",
    "{topic_name}\n",
    "- {keywords}\n",
    "\"\"\".strip()\n",
    "\n",
    "topic_seed_strs = []\n",
    "for topic_name, keywords in TOPIC_SEEDS.items():\n",
    "    topic_seed_strs.append(\n",
    "        TOPIC_SEED_STR.format(\n",
    "            topic_name=topic_name,\n",
    "            keywords=\"\\n- \".join(keywords),\n",
    "        )\n",
    "    )\n",
    "\n",
    "TOPIC_SEED_LIST_STR = \"\\n\\n\".join(topic_seed_strs)\n",
    "\n",
    "# Classify all comments\n",
    "classified_comments = []\n",
    "for _, row in tqdm(\n",
    "    meeting_comments.sample(50).iterrows(),\n",
    "    desc=\"Classifying Comments\",\n",
    "    total=len(meeting_comments),\n",
    "):\n",
    "    # Sleep to avoid rate limiting\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    try:\n",
    "        # Send the message\n",
    "        message = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20240620\",\n",
    "            max_tokens=1000,\n",
    "            temperature=0,\n",
    "            system=SYSTEM_MESSAGE_PROMPT,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": USER_MESSAGE_PROMPT.format(\n",
    "                                topic_list_str=TOPIC_SEED_LIST_STR,\n",
    "                                public_comment=row[\"text\"],\n",
    "                            ),\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Unpack the content\n",
    "        content = message.content[0].text\n",
    "\n",
    "        # Parse the XML\n",
    "        root = ET.fromstring(content)\n",
    "\n",
    "        # Print for logging\n",
    "        # print(ET.tostring(root, encoding=\"utf-8\").decode(\"utf-8\"))\n",
    "\n",
    "        # Get the topic, throw away the reasoning\n",
    "        topic = root.find(\"topic\").text\n",
    "\n",
    "        # Add to the row\n",
    "        classified_comments.append(\n",
    "            {\n",
    "                **row.to_dict(),\n",
    "                \"topic\": topic,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_comments_df = pd.DataFrame(classified_comments)\n",
    "classified_comments_df.to_csv(\"claude-35-sonnet-classifications.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all undefined topics\n",
    "plotting_comments = meeting_comments[meeting_comments[\"topic_name\"] != \"Undefined\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percent of Comments by Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep this version of \"all of the data\"\n",
    "# For the next two versions of the heatmap,\n",
    "# fit the topic model on the training and valid set and apply on the inferred set\n",
    "\n",
    "# Add one version of the heatmap that uses the \"inferred\" with the true comments\n",
    "# Add one version of the heatmap that uses the \"inferred\" with the inferred comments\n",
    "# Ignore the \"Commenter\" Speaker Role check for the \"inferred\" inferred data\n",
    "\n",
    "# Get keywords for each major topic\n",
    "# And a few representative comments\n",
    "# Flip the axes and make versions of the plots with the keywords\n",
    "\n",
    "# Create topic name and city count dataframe\n",
    "topic_city_counts = (\n",
    "    plotting_comments.groupby([\"city_name\", \"topic_name\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"comment_count\")\n",
    ")\n",
    "\n",
    "# Convert to percentage and store in new frame\n",
    "topic_city_counts[\"comment_pct\"] = topic_city_counts.groupby(\"city_name\")[\n",
    "    \"comment_count\"\n",
    "].transform(lambda x: x / x.sum())\n",
    "\n",
    "# Heatmap of topic percentage per month per city\n",
    "sns.heatmap(\n",
    "    data=topic_city_counts.pivot(\n",
    "        index=\"city_name\", columns=\"topic_name\", values=\"comment_pct\"\n",
    "    ),\n",
    "    annot=True,\n",
    "    fmt=\".1f\",\n",
    ")\n",
    "_ = plt.xticks(rotation=45, ha=\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety Checks on Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM plot with F1 of model + number of comments\n",
    "city_name_f1_lut = {\n",
    "    \"Ann Arbor\": 0.854,\n",
    "    \"Royal Oak\": 0.781,\n",
    "    \"Jackson\": 0.789,\n",
    "    \"Lansing\": 0.627,\n",
    "    \"Seattle\": 0.957,\n",
    "    \"Oakland\": 0.719,\n",
    "}\n",
    "\n",
    "city_name_k_alpha_lut = {\n",
    "    \"Ann Arbor\": 0.918,\n",
    "    \"Royal Oak\": 0.931,\n",
    "    \"Jackson\": 0.876,\n",
    "    \"Lansing\": 0.953,\n",
    "    \"Seattle\": 0.982,\n",
    "    \"Oakland\": 0.900,\n",
    "}\n",
    "\n",
    "# Create dataframe of city_name, f1, comment_count, and entropy(topic_percents)\n",
    "city_f1_entropy_df = []\n",
    "for city_name, city_df in topic_city_counts.groupby(\"city_name\"):\n",
    "    if city_name not in city_name_f1_lut:\n",
    "        continue\n",
    "\n",
    "    # Get the f1\n",
    "    f1 = city_name_f1_lut[city_name]\n",
    "    k_alpha = city_name_k_alpha_lut[city_name]\n",
    "\n",
    "    # Get the comment count\n",
    "    comment_count = city_df[\"comment_count\"].sum()\n",
    "\n",
    "    # Get the entropy\n",
    "    topic_percents = city_df[\"comment_pct\"].values\n",
    "    entropy_val = entropy(topic_percents)\n",
    "\n",
    "    city_f1_entropy_df.append(\n",
    "        {\n",
    "            \"city_name\": city_name,\n",
    "            \"model_f1\": f1,\n",
    "            \"k_alpha\": k_alpha,\n",
    "            \"comment_count\": comment_count,\n",
    "            \"topic_entropy\": entropy_val,\n",
    "        }\n",
    "    )\n",
    "\n",
    "city_f1_entropy_df = pd.DataFrame(city_f1_entropy_df)\n",
    "city_f1_entropy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model F1 by Number of Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM plot with F1 of model + number of comments\n",
    "_ = sns.lmplot(\n",
    "    data=city_f1_entropy_df,\n",
    "    x=\"comment_count\",\n",
    "    y=\"model_f1\",\n",
    "    # hue=\"city_name\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model F1 by Entropy of Topic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM plot with F1 and entropy\n",
    "_ = sns.lmplot(\n",
    "    data=city_f1_entropy_df,\n",
    "    x=\"topic_entropy\",\n",
    "    y=\"model_f1\",\n",
    "    # hue=\"city_name\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM plot with F1 and entropy\n",
    "_ = sns.lmplot(\n",
    "    data=city_f1_entropy_df,\n",
    "    x=\"k_alpha\",\n",
    "    y=\"model_f1\",\n",
    "    # hue=\"city_name\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM plot with F1 and entropy\n",
    "_ = sns.lmplot(\n",
    "    data=city_f1_entropy_df,\n",
    "    x=\"k_alpha\",\n",
    "    y=\"topic_entropy\",\n",
    "    # hue=\"city_name\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
